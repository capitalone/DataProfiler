{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69d960e",
   "metadata": {},
   "source": [
    "# Adding new model to the Data Labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390d324",
   "metadata": {},
   "source": [
    "In this example, we define a new model to be used with the Data Labeler component of the Data Profiler. In particular, a character-level LSTM model is implemented, then integrated into the DataLabeler pipeline to be trained with a tabular dataset.\n",
    "\n",
    "First, let's import the libraries needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60df21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "sys.path.insert(0, '..')\n",
    "import dataprofiler as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93242e08",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb197a0",
   "metadata": {},
   "source": [
    "We use the aws honeypot dataset in the test folder for this example. First, look at the data using the Data Reader class of the Data Profiler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dp.Data(\"../dataprofiler/tests/data/csv/aws_honeypot_marx_geo.csv\")\n",
    "df_data = data.data\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aaeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column 'comment' is changed to UNKNOWN, as the data labeler requires at least one column with label UNKNOWN\n",
    "df = data.data.rename({'comment': 'UNKNOWN'}, axis=1)\n",
    "\n",
    "# split data to training and test set\n",
    "split_ratio = 0.2\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "data_train = df[:int((1 - split_ratio) * len(df))]\n",
    "data_test = df[int((1 - split_ratio) * len(df)):]\n",
    "\n",
    "# train a new data labeler with column names as labels\n",
    "if not os.path.exists('data_labeler_saved'):\n",
    "    os.makedirs('data_labeler_saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c4a16",
   "metadata": {},
   "source": [
    "## Implement a new LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9819b",
   "metadata": {},
   "source": [
    "This model is inherited from CharacterLevelCnnModel class, with some modifications the following functions\n",
    "\n",
    "__init__: to add new parameters for lstm model\n",
    "\n",
    "_validate_parameters: to check new parameters for lstm model\n",
    "\n",
    "_construct_model: to construct new architecture for lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28984267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataprofiler.labelers.character_level_cnn_model import CharacterLevelCnnModel, F1Score, \\\n",
    "                                                            create_glove_char, build_embd_dictionary\n",
    "from dataprofiler.labelers.base_model import BaseModel\n",
    "\n",
    "class CharacterLevelLstmModel(CharacterLevelCnnModel):\n",
    "\n",
    "    # boolean if the label mapping requires the mapping for index 0 reserved\n",
    "    requires_zero_mapping = True\n",
    "\n",
    "    def __init__(self, label_mapping=None, parameters=None):\n",
    "        \"\"\"\n",
    "        LSTM Model Initializer. initialize epoch_id\n",
    "        \"\"\"\n",
    "                \n",
    "        # parameter initialization\n",
    "        if not parameters:\n",
    "            parameters = {}\n",
    "        parameters.setdefault('max_length', 3400)\n",
    "        parameters.setdefault('max_char_encoding_id', 127)\n",
    "        parameters.setdefault('dim_embed', 64)\n",
    "        parameters.setdefault('size_fc', [32, 32])\n",
    "        parameters.setdefault('dropout', 0.1)\n",
    "        parameters.setdefault('size_lstm', [64])\n",
    "        parameters.setdefault('rec_dropout', 0.1)\n",
    "        parameters.setdefault('activation', \"tanh\")\n",
    "        parameters.setdefault('recurrent_activation', \"sigmoid\")\n",
    "        parameters.setdefault('default_label', \"UNKNOWN\")\n",
    "        parameters['pad_label'] = 'PAD'\n",
    "        self._epoch_id = 0\n",
    "\n",
    "        # reconstruct flags for model\n",
    "        self._model_num_labels = 0\n",
    "        self._model_default_ind = -1\n",
    "\n",
    "        BaseModel.__init__(self, label_mapping, parameters)\n",
    "\n",
    "    def _validate_parameters(self, parameters):\n",
    "        \"\"\"\n",
    "        Validate the parameters sent in. Raise error if invalid parameters are\n",
    "        present.\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        list_of_necessary_params = ['max_length', 'max_char_encoding_id',\n",
    "                                    'dim_embed', 'size_fc', 'dropout',\n",
    "                                    'size_lstm', 'rec_dropout', 'activation', \n",
    "                                    'recurrent_activation', 'default_label', \n",
    "                                    'pad_label']\n",
    "        # Make sure the necessary parameters are present and valid.\n",
    "        for param in parameters:\n",
    "            if param in ['max_length', 'max_char_encoding_id', 'dim_embed',\n",
    "                         'size_conv']:\n",
    "                if not isinstance(parameters[param], (int, float)) \\\n",
    "                        or parameters[param] < 0:\n",
    "                    errors.append(param + \" must be a valid integer or float \"\n",
    "                                          \"greater than 0.\")\n",
    "            elif param in ['dropout', 'rec_dropout']:\n",
    "                if not isinstance(parameters[param], (int, float)) \\\n",
    "                        or parameters[param] < 0 or parameters[param] > 1:\n",
    "                    errors.append(param + \" must be a valid integer or float \"\n",
    "                                          \"from 0 to 1.\")\n",
    "            elif param == 'size_fc' or param == 'size_lstm':\n",
    "                if not isinstance(parameters[param], list) \\\n",
    "                        or len(parameters[param]) == 0:\n",
    "                    errors.append(param + \" must be a non-empty list of \"\n",
    "                                          \"integers.\")\n",
    "                else:\n",
    "                    for item in parameters[param]:\n",
    "                        if not isinstance(item, int):\n",
    "                            errors.append(param + \" must be a non-empty \"\n",
    "                                                  \"list of integers.\")\n",
    "                            break\n",
    "            elif param in ['default_label', 'activation', 'recurrent_activation']:\n",
    "                if not isinstance(parameters[param], str):\n",
    "                    error = str(param) + \" must be a string.\"\n",
    "                    errors.append(error)\n",
    "\n",
    "        # Error if there are extra parameters thrown in\n",
    "        for param in parameters:\n",
    "            if param not in list_of_necessary_params:\n",
    "                errors.append(param + \" is not an accepted parameter.\")\n",
    "        if errors:\n",
    "            raise ValueError('\\n'.join(errors))\n",
    "\n",
    "    def _construct_model(self):\n",
    "        \"\"\"\n",
    "        Model constructor for the data labeler. This also serves as a weight\n",
    "        reset.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        num_labels = self.num_labels\n",
    "        default_ind = self.label_mapping[self._parameters['default_label']]\n",
    "\n",
    "        # Reset model\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # generate glove embedding\n",
    "        create_glove_char(self._parameters['dim_embed'])\n",
    "\n",
    "        # generate model\n",
    "        self._model = tf.keras.models.Sequential()\n",
    "\n",
    "        # default parameters\n",
    "        max_length = self._parameters['max_length']\n",
    "        max_char_encoding_id = self._parameters['max_char_encoding_id']\n",
    "\n",
    "        # Encoding layer\n",
    "        def encoding_function(input_str):\n",
    "            char_in_vector = CharacterLevelLstmModel._char_encoding_layer(\n",
    "                input_str, max_char_encoding_id, max_length)\n",
    "            return char_in_vector\n",
    "\n",
    "        self._model.add(tf.keras.layers.Input(shape=(None,), dtype=tf.string))\n",
    "\n",
    "        self._model.add(\n",
    "            tf.keras.layers.Lambda(encoding_function,\n",
    "                                   output_shape=tuple([max_length])))\n",
    "\n",
    "        # Create a pre-trained weight matrix\n",
    "        # character encoding indices range from 0 to max_char_encoding_id,\n",
    "        # we add one extra index for out-of-vocabulary character\n",
    "        embed_file = os.path.join(\n",
    "            \"../dataprofiler/labelers\", \"embeddings/glove-reduced-{}D.txt\".format(\n",
    "                self._parameters['dim_embed']))\n",
    "        embedding_matrix = np.zeros((max_char_encoding_id + 2,\n",
    "                                     self._parameters['dim_embed']))\n",
    "        embedding_dict = build_embd_dictionary(embed_file)\n",
    "\n",
    "        input_shape = tuple([max_length])\n",
    "        # Fill in the weight matrix: let pad and space be 0s\n",
    "        for ascii_num in range(max_char_encoding_id):\n",
    "            if chr(ascii_num) in embedding_dict:\n",
    "                embedding_matrix[ascii_num + 1] = embedding_dict[chr(ascii_num)]\n",
    "\n",
    "        self._model.add(tf.keras.layers.Embedding(\n",
    "            max_char_encoding_id + 2,\n",
    "            self._parameters['dim_embed'],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=input_shape[0],\n",
    "            trainable=True))\n",
    "            \n",
    "        # Add the lstm layers\n",
    "        for size in self._parameters['size_lstm']:\n",
    "            self._model.add(\n",
    "                tf.keras.layers.LSTM(units=size, \n",
    "                                     recurrent_dropout=self._parameters['rec_dropout'], \n",
    "                                     activation=self._parameters['activation'],\n",
    "                                     recurrent_activation=self._parameters['recurrent_activation'],\n",
    "                                     return_sequences=True))\n",
    "            if self._parameters['dropout']:\n",
    "                self._model.add(tf.keras.layers.Dropout(self._parameters['dropout']))\n",
    "\n",
    "        # Add the fully connected layers\n",
    "        for size in self._parameters['size_fc']:\n",
    "            self._model.add(\n",
    "                tf.keras.layers.Dense(units=size, activation='relu'))\n",
    "            if self._parameters['dropout']:\n",
    "                self._model.add(\n",
    "                    tf.keras.layers.Dropout(self._parameters['dropout']))\n",
    "\n",
    "        # Add the final Softmax layer\n",
    "        self._model.add(\n",
    "            tf.keras.layers.Dense(num_labels, activation='softmax'))\n",
    "\n",
    "        # Output the model into a .pb file for TensorFlow\n",
    "        argmax_layer = tf.keras.backend.argmax(self._model.output)\n",
    "\n",
    "        # Create confidence layers\n",
    "        final_predicted_layer = CharacterLevelLstmModel._argmax_threshold_layer(\n",
    "            num_labels, threshold=0.0, default_ind=default_ind)\n",
    "\n",
    "        argmax_outputs = self._model.outputs + \\\n",
    "                         [argmax_layer,\n",
    "                          final_predicted_layer(argmax_layer, self._model.output)]\n",
    "        self._model = tf.keras.Model(self._model.inputs, argmax_outputs)\n",
    "\n",
    "        # Compile the model\n",
    "        softmax_output_layer_name = self._model.outputs[0].name.split('/')[0]\n",
    "        losses = {softmax_output_layer_name: \"categorical_crossentropy\"}\n",
    "\n",
    "        # use f1 score metric\n",
    "        f1_score_training = F1Score(num_classes=num_labels, average='micro')\n",
    "        metrics = {softmax_output_layer_name: ['acc', f1_score_training]}\n",
    "\n",
    "        self._model.compile(loss=losses,\n",
    "                            optimizer=\"adam\",\n",
    "                            metrics=metrics)\n",
    "\n",
    "        self._epoch_id = 0\n",
    "        self._model_num_labels = num_labels\n",
    "        self._model_default_ind = default_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447f70b",
   "metadata": {},
   "source": [
    "## Integrate the LSTM model to the DataLabeler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cb541",
   "metadata": {},
   "source": [
    "The above LSTM model is loaded along with Preprocessor and Postprocessor to build a new DataLabeler, which is then trained on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d553606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from the given dataset\n",
    "value_label_df = data_train.reset_index(drop=True).melt()\n",
    "value_label_df.columns = [1, 0]  # labels=1, values=0 in that order\n",
    "value_label_df = value_label_df.astype(str)\n",
    "labels = value_label_df[1].unique().tolist()\n",
    "\n",
    "# build new data labeler with preprocessor, new lstm model, and postprocessor\n",
    "preprocessor = dp.labelers.data_processing.StructCharPreprocessor()\n",
    "model = CharacterLevelLstmModel(labels)\n",
    "postprocessor = dp.labelers.data_processing.StructCharPostprocessor()\n",
    "data_labeler = dp.labelers.base_data_labeler.TrainableDataLabeler.load_with_components(preprocessor, model, postprocessor)\n",
    "\n",
    "# train the data labeler\n",
    "save_dirpath=\"data_labeler_saved\"\n",
    "epochs=2\n",
    "data_labeler.fit(\n",
    "    x=value_label_df[0], y=value_label_df[1], labels=labels, epochs=epochs)\n",
    "if save_dirpath:\n",
    "    data_labeler.save_to_disk(save_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a4280",
   "metadata": {},
   "source": [
    "The trained Data Labeler is then used by the Data Profiler to provide the prediction on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80243264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the data labeler object\n",
    "profile_options = dp.ProfilerOptions()\n",
    "profile_options.set({\"text.is_enabled\": False, \n",
    "                     \"int.is_enabled\": False, \n",
    "                     \"float.is_enabled\": False, \n",
    "                     \"order.is_enabled\": False, \n",
    "                     \"category.is_enabled\": False, \n",
    "                     \"datetime.is_enabled\": False,})\n",
    "profile_options.set({'data_labeler.data_labeler_object': data_labeler})\n",
    "profile = dp.Profiler(data_test, profiler_options=profile_options)\n",
    "\n",
    "# get the prediction from the data profiler\n",
    "def get_structured_results(results):\n",
    "    columns = []\n",
    "    predictions = []\n",
    "    for col in results['data_stats']:\n",
    "        columns.append(col)\n",
    "        predictions.append(results['data_stats'][col]['data_label'])\n",
    "\n",
    "    df_results = pd.DataFrame({'Column': columns, 'Prediction': predictions})\n",
    "    return df_results\n",
    "\n",
    "results = profile.report()\n",
    "print(get_structured_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc2067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
