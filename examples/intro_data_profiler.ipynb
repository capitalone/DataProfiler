{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60af6856",
   "metadata": {},
   "source": [
    "# DataProfiler - What's in your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297611c",
   "metadata": {},
   "source": [
    "This introductory jupyter notebook demonstrates the basic usages of the DataProfiler. The library is designed in the way that users can easily get the statistics and other detailed information about the input datasets with just several lines of code. DataProfiler provides multiple data classes that can handle different types of data. In addition, users are given various options to skip some properties if not needed while profiling their datasets. The last key feature covered in this example is the ability to allow users to update their profilers from multiple batches of the large datasets, or merge multiple profilers which is suitable for the distributed computing environment. In particular, this example covers the followings:\n",
    "\n",
    "    - Basic usage of DataProfiler\n",
    "    - Profiler options\n",
    "    - Data reader class\n",
    "    - Update profiles and merge profiles\n",
    "\n",
    "First, let's import the libraries needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b774f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '..')\n",
    "import dataprofiler as dp\n",
    "\n",
    "data_path = \"../dataprofiler/tests/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054a3df",
   "metadata": {},
   "source": [
    "## Basic examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7083d8",
   "metadata": {},
   "source": [
    "This section shows the basic example of DataProfiler. A CSV dataset is read using the data reader, then the returned data is given to the DataProfiler to obtain the properties and stastistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data reader to read input data\n",
    "#data = dp.Data(\"../dataprofiler/tests/data/csv/aws_honeypot_marx_geo.csv\")\n",
    "data = dp.Data(os.path.join(data_path, \"csv/aws_honeypot_marx_geo.csv\"))\n",
    "print(data.data.head())\n",
    "\n",
    "# run data profiler and get the report\n",
    "profile = dp.Profiler(data)\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "\n",
    "# print the report\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ffdf6",
   "metadata": {},
   "source": [
    "The report includes `global_stats` and `data_stats` for the given dataset. The former contains overall properties of the data such as number of rows/columns, null ratio, duplicate ratio, while the later contains specific properties and statistics for each column such as min, max, mean, variance, etc. In this example, the `compact` format of the report is used to shorten the full list of the results. To get more results related to detailed predictions at the entity level from the DataLabeler component or histogram results, the format `pretty` should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81381be5",
   "metadata": {},
   "source": [
    "In addition to reading the input data from multiple file types, DataProfiler allows the input data as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3519d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run data profiler and get the report\n",
    "my_dataframe = pd.DataFrame([[1, 2.0],[1, 2.2],[-1, 3]], columns=[\"col_int\", \"col_float\"])\n",
    "profile = dp.Profiler(my_dataframe)\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "\n",
    "# Print the report\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bf9d1",
   "metadata": {},
   "source": [
    "## Profiler options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97551a0c",
   "metadata": {},
   "source": [
    "DataProfiler can run several selected components if needed. For example, if the users only want the statistics information, they may turn off the DataLabeler functionality. Below, let's remove the histogram and data labeler component while running DataProfiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e70efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_options = dp.ProfilerOptions()\n",
    "profile_options.set({\"histogram.is_enabled\": False,\n",
    "                     \"data_labeler.is_enabled\": False,})\n",
    "\n",
    "profile = dp.Profiler(data, profiler_options=profile_options)\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "\n",
    "# Print the report\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c121509",
   "metadata": {},
   "source": [
    "## Data reader class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a927579",
   "metadata": {},
   "source": [
    "DataProfiler can detect multiple file types including CSV, JSON, Parquet, AVRO, and text. The example below shows that it successfully detects data types from multiple categories regardless of the file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data reader to read input data with different file types\n",
    "csv_files = [\n",
    "    \"csv/aws_honeypot_marx_geo.csv\",\n",
    "    \"csv/all-strings-skip-header-author.csv\", # csv files with the author/description on the first line\n",
    "    \"csv/sparse-first-and-last-column-empty-first-row.txt\", # csv file with the .txt extension\n",
    "]\n",
    "json_files = [\n",
    "    \"json/complex_nested.json\",\n",
    "    \"json/honeypot_intentially_mislabeled_file.csv\", # json file with the .csv extension\n",
    "]\n",
    "parquet_files = [\n",
    "    \"parquet/nation.dict.parquet\",\n",
    "    \"parquet/nation.plain.intentionally_mislabled_file.csv\", # parquet file with the .csv extension\n",
    "]\n",
    "avro_files = [\n",
    "    \"avro/userdata1.avro\",\n",
    "    \"avro/userdata1_intentionally_mislabled_file.json\", # avro file with the .json extension\n",
    "]\n",
    "text_files = [\n",
    "    \"txt/discussion_reddit.txt\",\n",
    "]\n",
    "\n",
    "all_files = {\n",
    "    \"csv\": csv_files,\n",
    "    \"json\": json_files,\n",
    "    \"parquet\": parquet_files,\n",
    "    \"avro\": avro_files,\n",
    "    \"text\": text_files\n",
    "}\n",
    "\n",
    "for file_type in all_files:\n",
    "    print(file_type)\n",
    "    for file in all_files[file_type]:\n",
    "        data = dp.Data(os.path.join(data_path, file))\n",
    "        print(\"{:<85} {:<15}\".format(file, data.data_type))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285366f",
   "metadata": {},
   "source": [
    "The `Data` class detects the file type and uses one of the following classes: `CSVData`, `JSONData`, `ParquetData`, `AVROData`, `TextData`. Users can call these specific classes directly if desired. For example, below we provide a collection of data with different types, each of them is processed by the corresponding data class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6adfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use individual data reader classes\n",
    "from dataprofiler.data_readers.csv_data import CSVData\n",
    "from dataprofiler.data_readers.json_data import JSONData\n",
    "from dataprofiler.data_readers.parquet_data import ParquetData\n",
    "from dataprofiler.data_readers.avro_data import AVROData\n",
    "from dataprofiler.data_readers.text_data import TextData\n",
    "\n",
    "csv_files = \"csv/aws_honeypot_marx_geo.csv\"\n",
    "json_files = \"json/complex_nested.json\"\n",
    "parquet_files = \"parquet/nation.dict.parquet\"\n",
    "avro_files = \"avro/userdata1.avro\"\n",
    "text_files = \"txt/discussion_reddit.txt\"\n",
    "\n",
    "all_files = {\n",
    "    \"csv\": [csv_files, CSVData],\n",
    "    \"json\": [json_files, JSONData],\n",
    "    \"parquet\": [parquet_files, ParquetData],\n",
    "    \"avro\": [avro_files, AVROData],\n",
    "    \"text\": [text_files, TextData],\n",
    "}\n",
    "\n",
    "for file_type in all_files:\n",
    "    file, data_reader = all_files[file_type]\n",
    "    data = data_reader(os.path.join(data_path, file))\n",
    "    print(\"File name {}\\n\".format(file))\n",
    "    if file_type == \"text\":\n",
    "        print(data.data[0][:1000]) # print the first 1000 characters\n",
    "    else:\n",
    "        print(data.data)\n",
    "    print('===============================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f44679",
   "metadata": {},
   "source": [
    "## Update profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917fc28f",
   "metadata": {},
   "source": [
    "One of the interesting features of the DataProfiler is the ability to update profiles from batches of data, which is then makes DataProfiler applicable for data streaming usage. In this section, the original dataset is separated into two batches with equal size. Each batch is then updated with DataProfiler sequentially.  \n",
    "\n",
    "After the update, we expect the resulted profiles give the same statistics as the profiles updated from the full dataset. We will verify that through some properties in `global_stats` of the profiles including `column_count`, `row_count`, `row_is_null_ratio`, `duplicate_row_count`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the input data and devide it into two equal halves\n",
    "data = dp.Data(os.path.join(data_path, \"csv/aws_honeypot_marx_geo.csv\"))\n",
    "df = data.data\n",
    "df1 = df.iloc[:int(len(df)/2)]\n",
    "df2 = df.iloc[int(len(df)/2):]\n",
    "\n",
    "# Update the profile with the first half\n",
    "profile = dp.Profiler(df1)\n",
    "\n",
    "# Update the profile with the second half\n",
    "profile.update_profile(df2)\n",
    "\n",
    "# Update profile with the full dataset\n",
    "profile_full = dp.Profiler(df)\n",
    "\n",
    "# check results of the updated profile\n",
    "report = profile.report()\n",
    "report_full = profile_full.report()\n",
    "\n",
    "list_prop = [\"column_count\", \"row_count\", \"row_is_null_ratio\", \"duplicate_row_count\"]\n",
    "print(\"\\n\\n\")\n",
    "for prop in list_prop:\n",
    "    print(\"{:<20} {:<15} {:<15}\".format(prop, report[\"global_stats\"][prop], report_full[\"global_stats\"][prop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ea2c2",
   "metadata": {},
   "source": [
    "## Merge profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62375e59",
   "metadata": {},
   "source": [
    "In addition to the profile update, DataProfiler provides the merging functionality which allows users to combine the profiles updated from multiple locations. This enables DataProfiler to be used in a distributed computing environment. Below, we assume that the two aforementioned halves of the original dataset come from two different machines. Each of them is then updated with the DataProfiler on the same machine, then the resulted profiles are merged.\n",
    "\n",
    "As with the profile upate, we expect the merged profiles give the same statistics as the profiles updated from the full dataset. We will verify that through some properties in `global_stats` of the profiles including `column_count`, `row_count`, `row_is_null_ratio`, `duplicate_row_count`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the profile with the first half\n",
    "profile1 = dp.Profiler(df1)\n",
    "\n",
    "# Update the profile with the second half\n",
    "profile2 = dp.Profiler(df2)\n",
    "\n",
    "# merge profiles\n",
    "profile_merge = profile1 + profile2\n",
    "\n",
    "# check results of the merged profile\n",
    "report_merge = profile_merge.report()\n",
    "report_full = profile_full.report()\n",
    "\n",
    "list_prop = [\"column_count\", \"row_count\", \"row_is_null_ratio\", \"duplicate_row_count\"]\n",
    "print(\"\\n\\n\")\n",
    "for prop in list_prop:\n",
    "    print(\"{:<20} {:<15} {:<15}\".format(prop, report_merge[\"global_stats\"][prop], report_full[\"global_stats\"][prop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77efc73",
   "metadata": {},
   "source": [
    "### More on the profile merge for the statistics update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad486c",
   "metadata": {},
   "source": [
    "Beside the global properties of the profiles, the following example demonstrates that the merged profile and the profile updated from the full dataset obtain the same results for statistics of the given dataset. First, let's review the total list of statistics for a selected column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85208fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select `int_col` column from the data to show the statistics update\n",
    "report = profile_full.report()\n",
    "selected_col = \"int_col\"\n",
    "stats = report[\"data_stats\"][selected_col][\"statistics\"]\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20f1cf",
   "metadata": {},
   "source": [
    "Now, let's choose several statistics, `min`, `max`, `mean`, `variance`, `stddev`, to check the profile merging. We'll see that the statistics update from the merging profile is the same as from the profile updated from the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_stats = [\"min\", \"max\", \"mean\", \"variance\", \"stddev\"]\n",
    "result_stats = {stat:[] for stat in list_stats}\n",
    "list_profiles = [profile1, profile2, profile_merge, profile_full]\n",
    "selected_col = \"int_col\"\n",
    "\n",
    "for profile in list_profiles:\n",
    "    report = profile.report()\n",
    "    stats = report[\"data_stats\"][selected_col][\"statistics\"]\n",
    "    for stat in result_stats:\n",
    "        result_stats[stat].append(stats[stat])\n",
    "result_stats = pd.concat([pd.DataFrame({\"Profile\": [\"profile1\", \"profile2\", \"profile_merge\", \"profile_full\"]}), \n",
    "                          pd.DataFrame(result_stats)], axis=1)\n",
    "result_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda33ef5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have walked through some basic examples of DataProfiler usage, with different input data types and profiling options. We also work with update and merging functionality of the DataProfiler, which make it applicable for data streaming and distributed environment. Interested users can try with different datasets and functionalities as desired."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
