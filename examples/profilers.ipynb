{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a86a1e3",
   "metadata": {},
   "source": [
    "# DataProfiler - Profilers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe7e3f",
   "metadata": {},
   "source": [
    "Data profiling is the process of examining a dataset and collecting statistical or informational summaries about said dataset.\n",
    "\n",
    "The Profilers inside the DataProfiler is designed to calculate multiple statistics, make predictions on the entities inside a given column / key-value store (via the Labeler) and generally determine informational summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, '..')\n",
    "import dataprofiler as dp\n",
    "\n",
    "data_path = \"../dataprofiler/tests/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8a7e4",
   "metadata": {},
   "source": [
    "## Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01713c71",
   "metadata": {},
   "source": [
    "One of the primary purposes of the Profiler are to quickly identify what is in the dataset. This can be useful for analyzing a dataset prior to use or determining which columns could be useful for a given purpose.\n",
    "\n",
    "In terms of reporting, there are multiple reporting options:\n",
    "\n",
    "* **Pretty**: Floats are rounded to four decimal places, and lists are shortened.\n",
    "* **Compact**: Similar to pretty, but removes detailed statistics such as runtimes, label probabilities, index locations of null types, etc.\n",
    "* **Serializable**: Output is json serializable and not prettified\n",
    "* **Flat**: Nested Output is returned as a flattened dictionary\n",
    "\n",
    "The **Pretty** and **Compact** reports are the two most commonly used reports and includes `global_stats` and `data_stats` for the given dataset. \n",
    "\n",
    "`global_stats` contains overall properties of the data such as number of rows/columns, null ratio, duplicate ratio:\n",
    "\n",
    "```\n",
    "\"global_stats\": {\n",
    "    \"samples_used\": int,\n",
    "    \"column_count\": int,\n",
    "    \"row_count\": int,\n",
    "    \"row_has_null_ratio\": float,\n",
    "    \"row_is_null_ratio\": float,    \n",
    "    \"unique_row_ratio\": float,\n",
    "    \"duplicate_row_count\": int,\n",
    "    \"file_type\": string,\n",
    "    \"encoding\": string,\n",
    "}\n",
    "```\n",
    "\n",
    "`data_stats` contains specific properties and statistics for each column such as min, max, mean, variance, etc.\n",
    "\n",
    "\n",
    "```\n",
    "\"data_stats\": {\n",
    "    <column name>: {\n",
    "        \"column_name\": string,\n",
    "        \"data_type\": string,\n",
    "        \"data_label\": string,\n",
    "        \"categorical\": bool,\n",
    "        \"order\": string,\n",
    "        \"samples\": list(str),\n",
    "        \"statistics\": {\n",
    "            \"sample_size\": int,\n",
    "            \"null_count\": int,\n",
    "            \"null_types\": list(string),\n",
    "            \"null_types_index\": {\n",
    "                string: list(int)\n",
    "            },\n",
    "            \"data_type_representation\": [string, list(string)],\n",
    "            \"min\": [null, float],\n",
    "            \"max\": [null, float],\n",
    "            \"mean\": float,\n",
    "            \"variance\": float,\n",
    "            \"stddev\": float,\n",
    "            \"histogram\": { \n",
    "                \"bin_counts\": list(int),\n",
    "                \"bin_edges\": list(float),\n",
    "            },\n",
    "            \"quantiles\": {\n",
    "                int: float\n",
    "            }\n",
    "            \"vocab\": list(char),\n",
    "            \"avg_predictions\": dict(float), \n",
    "            \"data_label_representation\": dict(float),\n",
    "            \"categories\": list(str),\n",
    "            \"unique_count\": int,\n",
    "            \"unique_ratio\": float,\n",
    "            \"precision\": {\n",
    "                'min': int,\n",
    "                'max': int,\n",
    "                'mean': float,\n",
    "                'var': float,\n",
    "                'std': float,\n",
    "                'sample_size': int,\n",
    "                'margin_of_error': float,\n",
    "                'confidence_level': float\t\t\n",
    "            },\n",
    "            \"times\": dict(float),\n",
    "            \"format\": string\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In the example, the `compact` format of the report is used to shorten the full list of the results. To get more results related to detailed predictions at the entity level from the DataLabeler component or histogram results, the format `pretty` should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dp.Data(os.path.join(data_path, \"csv/aws_honeypot_marx_geo.csv\"))\n",
    "profile = dp.Profiler(data)\n",
    "\n",
    "# Compact - A high level view, good for quick reviews\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10baa510",
   "metadata": {},
   "source": [
    "It should be noted, in addition to reading the input data from multiple file types, DataProfiler allows the input data as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run data profiler and get the report\n",
    "import pandas as pd\n",
    "my_dataframe = pd.DataFrame([[1, 2.0],[1, 2.2],[-1, 3]], columns=[\"col_int\", \"col_float\"])\n",
    "profile = dp.Profiler(my_dataframe)\n",
    "\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582af32",
   "metadata": {},
   "source": [
    "## Profiler options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98816781",
   "metadata": {},
   "source": [
    "DataProfiler can run several selected components if needed. For example, if the users only want the statistics information, they may turn off the DataLabeler functionality. Below, let's remove the histogram and data labeler component while running DataProfiler. \n",
    "\n",
    "Full list of options: https://capitalone.github.io/DataProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_options = dp.ProfilerOptions()\n",
    "profile_options.set({\n",
    "    \"histogram.is_enabled\": False,\n",
    "    \"data_labeler.is_enabled\": False\n",
    "})\n",
    "\n",
    "profile = dp.Profiler(data, profiler_options=profile_options)\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "\n",
    "# Print the report\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3fe8e2",
   "metadata": {},
   "source": [
    "### Updating Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2b150",
   "metadata": {},
   "source": [
    "Beyond just profiling, one of the unique aspects of the DataProfiler (as compared to pandas-profiling) is the ability to update the profiles. To update appropriately, the schema (columns / keys) must match appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and profile a CSV file\n",
    "data = dp.Data(os.path.join(data_path, \"csv/sparse-first-and-last-column-header-and-author.txt\"))\n",
    "profile = dp.Profiler(data)\n",
    "\n",
    "# Update the profile with new data:\n",
    "new_data = dp.Data(os.path.join(data_path, \"csv/sparse-first-and-last-column-skip-header.txt\"))\n",
    "profile.update_profile(new_data)\n",
    "\n",
    "# Report the compact version of the profile\n",
    "report  = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65a09f",
   "metadata": {},
   "source": [
    "### Merging Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d089e0b",
   "metadata": {},
   "source": [
    "Merging profiles are an alternative method for updating profiles. Particularly, multiple profiles can be generated seperately, then added together with a simple `+` command: `profile3 = profile1 + profile2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file with a schema\n",
    "data1 = dp.Data(os.path.join(data_path, \"csv/sparse-first-and-last-column-header-and-author.txt\"))\n",
    "profile1 = dp.Profiler(data1)\n",
    "\n",
    "# Load another CSV file with the same schema\n",
    "data2 = dp.Data(os.path.join(data_path, \"csv/sparse-first-and-last-column-skip-header.txt\"))\n",
    "profile2 = dp.Profiler(data2)\n",
    "\n",
    "# Merge the profiles\n",
    "profile3 = profile1 + profile2\n",
    "\n",
    "# Report the compact version of the profile\n",
    "report  = profile3.report(report_options={\"output_format\":\"compact\"})\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961de7d3",
   "metadata": {},
   "source": [
    "As you can see, the `update_profile` function and the `+` operator function similarly. The reason the `+` operator is important is that it's possible to *save and load profiles*, which we cover next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2869833",
   "metadata": {},
   "source": [
    "### Saving and Loading a Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15781300",
   "metadata": {},
   "source": [
    "Not only can the Profiler create and update profiles, it's also possible to save, load then manipulate profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7483bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file, with \",\" as the delimiter\n",
    "data = dp.Data(\"csv/sparse-first-and-last-column-header-and-author.txt\")\n",
    "\n",
    "# Read in profile and print results\n",
    "profile = dp.Profiler(data)\n",
    "profile.save(filepath=\"my_profile.pkl\")\n",
    "\n",
    "loaded_profile = dp.Profiler.load(\"my_profile.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eacfc2",
   "metadata": {},
   "source": [
    "With the ability to save and load profiles, profiles can be generated via multiple machines then merged. Further, profiles can be stored and later used in applications such as change point detction, synthetic data generaiton, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a multiple files via the Data class\n",
    "filenames = [\"csv/sparse-first-and-last-column-header-and-author.txt\",\n",
    "             \"csv/sparse-first-and-last-column-skip-header.txt\"]\n",
    "data_objects = []\n",
    "for filename in filenames:\n",
    "    data_objects.append(dp.Data(os.path.join(data_path, filename)))\n",
    "\n",
    "\n",
    "# Generate and save profiles\n",
    "for i in range(len(data_objects)):\n",
    "    profile = dp.Profiler(data_objects[i])\n",
    "    profile.save(filepath=\"data-\"+str(i)+\".pkl\")\n",
    "\n",
    "\n",
    "# Load profiles and add them together\n",
    "profile = None\n",
    "for i in range(len(data_objects)):\n",
    "    if profile is None:\n",
    "        profile = dp.Profiler.load(\"data-\"+str(i)+\".pkl\")\n",
    "    else:\n",
    "        profile += dp.Profiler.load(\"data-\"+str(i)+\".pkl\")\n",
    "\n",
    "\n",
    "# Report the compact version of the profile\n",
    "report = profile.report(report_options={\"output_format\":\"compact\"})\n",
    "print(json.dumps(report, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
